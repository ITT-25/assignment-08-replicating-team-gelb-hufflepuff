# üëÜ Gesture Mouse ‚Äì Markerbasierte Maussteuerung per Webcam  

Team Hufflepuff (David Ring & Marc Fl√∂ter), Universit√§t Regensburg, Interaktionstechniken und -technologien (SoSe2025)

<img src="figures/openposition.png" alt="User Interface der Gesture Mouse" width="500">

---

## Allgemein

Ziel des Assignments war, ein interessantes Interaktionssystem aus einer wissenschaftlichen Ver√∂ffentlichung nachzubauen. Im Rahmen dieser Abgabe wurde ein `Gesture Mouse` entwickelt, ein kamerabasiertes System, welches Nutzern erlaubt, mit farbigen Markern an den Fingerspitzen verschiedene Mausaktionen durchzuf√ºhren. Via Finger-Gesten vor einer Webcam erm√∂glicht die Anwendung Mausbewegung, Links- und Rechtsklick, Doppel-Linksklick sowie vertikales Scrollen ‚Äì ausschlie√ülich durch Gesten mit drei farbig markierten Fingern.

Diese Anwendung basiert auf dem Paper `"Design and Development of Hand Gesture Based
Virtual Mouse" von Shibly et al. (ICASERT 2019)`. Die hier implementierte Version orientiert sich eng an den im Paper beschriebenen Interaktionen, wurde jedoch in Python umgesetzt und um zus√§tzliche Funktionen wie Farbkalibrierung, Beschr√§nkung des Aktionsbereiches, On-Screen-Logging und Soundeffekte erweitert. Das Audiofeedback wurde generiert mit https://ttsmp3.com/ai.

---

## Benutzung

### Anforderungen

#### Hardware
- Webcam
- Drei farbige Marker an Zeigefinger, Mittelfinger und Daumen (z.B. rote Fingerkappen oder farbige Aufkleber)
- M√∂glichst neutraler Hintergrund

#### Software
Alle notwendigen Pakete sind in `requirements.txt` aufgelistet und k√∂nnen von dort installiert werden.

##### Externe Bibliotheken:
- `cv2` (OpenCV): Bildverarbeitung, Kamera, Konturenerkennung
- `numpy`: Array-Berechnungen, Schwellenwertvergleiche
- `pynput`: Steuerung der Maus
- `pyautogui`: Ermittlung der Bildschirmgr√∂√üe
- `pygame`: Soundeffekte

---

### Start und Kalibrierung

Die Anwendung wird direkt √ºber das Hauptskript gestartet:

```bash
python gesture_mouse.py
```


Per Commandline Argument k√∂nnen folgende Einstellungen beim Start festgelegt werden:
- `-v <ID>` setzt die Ger√§te-ID der zu verwendenden Webcam fest (default: 0)
- `-s <0/1>` Soundausgabe aktiv (1) oder inaktiv (0) (default: 1)

```bash
python gesture_mouse.py -v 0 -s 0
```

<img src="figures/startscreen.png" alt="Startbildschirm" width="300">

Beim Programmstart wird der Nutzer zun√§chst Aufgefordert, die Farbe der zu erkennenden Marker zu kalibrieren. Sobald der Nutzer `k` dr√ºckt, muss ein Marker einige Sekunden ruhig im Zentrum des Kamerabildes gehalten werden. Die erkannte Farbe wird als Zusatzinformation in einer Bildschirmecke angezeigt. Nun kann der Nutzer die Steuerung seines Desktop-Mauszeigers per Finger-Gestiken beginnen.

<img src="figures/calibrating.png" alt="Kalibrierung" width="300">

---

### Ansicht und Bedienung

Die Anwendung zeigt standardm√§√üig das Kamerabild der Webcam. Ein gr√ºner Rahmen markiert den Bereich, der zur Steuerung des Mauscursors aktiv ist. Unten links wird (je nach Einstellung) angezeigt, welche Geste gerade erkannt wird.
Wird ein Marker erkannt, erscheint um ihn in blau seine Bounding-Box (Anzeige einstellbar!). Wenn mehrere Marker erkannt werden, werden deren Mittelpunkte mit einer blauen Linie verbunden. Werden genau zwei Marker erkannt, erscheint mittig zwischen diesen ein roter Punkt, der den Mauscursor steuert.

#### Steuerungsgesten

| Geste                  | Aktion                      |
|------------------------|-----------------------------|
| **2 Marker, offen**    | Cursorbewegung (Mitte der beiden Marker)              |
| **2 Marker, zueinander f√ºhren (Pinch)** | Linksklick |
| **Pinch halten (5‚ÄØs)** | Doppel-Linksklick                 |
| **1 Marker sichtbar**  | Rechtsklick                 |
| **3 Marker, vertikale Bewegung** | Scrollen       |


<img src="figures/linksklick.png" alt="Linksklick durch Pinch-Geste (kurz halten f√ºr Doppelklick)" width="300">
<img src="figures/rechtsklick.png" alt="Rechtsklick durch 1 Finger Geste / Full Pinch" width="300">
<img src="figures/scroll.png" alt="Scrollen durch 3-Finger-Geste" width="300">

#### Weitere Steuerung

- `q` beendet die Anwendung
- `k` startet die Kalibrierung erneut

---

### Einstellungen

Die wichtigsten Parameter lassen sich direkt oben im Code anpassen:

#### Anzeigeoptionen
- `SHOW_CAM`: Kamerabild anzeigen
- `SHOW_BORDER`: Aktiven Tracking-Bereich f√ºr Mauscursorbewegung anzeigen
- `SHOW_BOUNDING_BOXES`: Rechtecke um erkannte Marker und Linien zwischen ihren Mittelpunkten anzeigen
- `SHOW_LOG`: Log-Overlay mit Statusmeldungen anzeigen

#### Marker- und Gestenerkennung
- `MIN_MARKER_SIZE`: Mindestgr√∂√üe einer Fl√§che der kalibrierten Farbe, um als Marker erkannt zu werden (default: 500 p)
- `BORDER_RATIO`: Abstand des Mauscursor-Trackingbereichs zum Rand (in % der Frameseitenl√§ngen, default: 0.1)
- `PINCH_CLICK_RATIO`: Verh√§ltnis f√ºr Linksklickausl√∂sung (default: 0.20)
- `PINCH_RESET_RATIO`: Verh√§ltnis f√ºr Linksklick-Ende (default: 0.35)
- `DOUBLE_CLICK_HOLD_TIME`: Haltezeit f√ºr Doppel-Linksklick (default: 5‚ÄØs)
- `MIN_Y_DIST_FOR_SCROLL`: Minimale y-Bewegung f√ºr Ausl√∂sen der Scroll-Geste (default: 10 p)
- `FRAMES_REQUIRED_FOR_RIGHT_CLICK`: Dauer bis Rechtsklick bei 1 Marker


#### Kalibrierung
- `HSV_TOLERANCE`: Hue-Farbtoleranzbereich um den kalibrierten Farbwert herum
- `CALIBRATION_TIME`: Dauer f√ºr Marker-Kalibrierung

Zudem k√∂nnen GUI-Designoptionen (Farben, Strichdicken etc.) per Konstante angepasst werden.

---
## Auswahl des Papers

### Literatursuche

Wir haben viele aktuelle und √§ltere Wissenschaftliche Ver√∂ffentlichungen in Bedachte gezogen, bevor wir uns f√ºr das Paper `"Design and Development of Hand Gesture Based Virtual Mouse" von Shibly et al. (ICASERT 2019)` entschieden haben. Die Idee, den Desktop-Mauszeiger auf irgendeine Art und Weise √ºber alternative Wege zu steuern klang f√ºr uns relevant, zu den bisherigen Kursinhalten passend und realistisch in 2 Wochen kosteng√ºnstig umzusetzen. Hierzu haben wir auch mehrere relevante Ver√∂ffentlichungen gefunden:

   - `Agrawal, Sandip, et al. "Using mobile phones to write in air." Proceedings of the 9th international conference on Mobile systems, applications, and services (2011)` : Steuerung der Desktop-Maus √ºber Handy-Accelerometer/Gyrometer-Daten w√§re durchaus denkbar, und die Umsetzung √ºber DIPPID w√§re bekannt gewesen. Dies w√§re kombinierbar gewesen mit dem Ansatz aus dem Paper `Ruiz, Jaime, and Yang Li. "DoubleFlip: a motion gesture delimiter for mobile interaction." Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (2011)`, welcher durch Gestiken wie Double-Flip des Smartphones bestimmte Aktionen triggern kann. Dieser kombinierte Ansatz k√∂nnte z. B. eine Bluetoothmaus ersetzen, mit dem Vorteil dass Leute ihr Handy fast immer bei sich haben. Nachteil an dem Ansatz ist, dass man sein Handy vielleicht eigentlich gerade f√ºr etwas anderes (z. B. gleichzeitige Videokonferenz) nutzen m√∂chte. 

   - `Wang, Jingtao, Shumin Zhai, and John Canny. "Camera phone based motion sensing: interaction techniques, applications and performance study." Proceedings of the 19th annual ACM symposium on User interface software and technology. 2006.` : In diesem relativ alten Paper wurde `tinyMotion`entwickelt, eine Anwendung die per Kamera-Bild-Auswertung die Bewegung der Kamera tracken kann. Das klang auch nach einer interessanten Idee f√ºr manche Anwendungszwecke, ist aber heutzutage mit Sensoren vermutlich genauer umsetzbar. Dennoch w√§re es eine denkbare Methode gewesen, einfache Maus-Bewegungen durch bewegen der Webcam zu steuern.

   - `Gupta, Sidhant, et al. "Soundwave: using the doppler effect to sense gestures." Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 2012.`: Mit Niedrigfrequenzt√∂nen und dem Doppler-Effekt konnten Gestiken erkannt werden. In dem Paper werden verschiedene Gestiken wie Scroll und Rotation mit 1-2 H√§nden vor dem Laptop robust in echtzeit ausgef√ºhrt. Das Prinzip dahinter fanden wir sehr clever, und man konnte damit sogar Tetris spielen, wie in ihrem Demo-Video zu sehen ist. Allerdings sind die Gestiken relativ gro√ü und m√∂glicherweise anstrengend √ºber l√§ngere Zeit durchzuf√ºhren. Zus√§tzlich ist das benutzen beider H√§ndle gleichzeitig relativ umst√§ndlich, verglichen mit L√∂sungen die nur die Fingerspitzen bewegen. Eine √§hnliche Sound-Basierte L√∂sung via Sonar wurde auch von `Nandakumar, R., Iyer, V., Tan, D., & Gollakota, S. (2016, May). Fingerio: Using active sonar for fine-grained finger tracking. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (pp. 1515-1525).` benutzt, welche die Finger via Echo-Effekte genau tracken konnte. Der Vorteil solch eines Systems ist, dass keine Direkte Sichtlinie ben√∂tigt wird, da der Ton auch aus versteckten Stellen heraus erkannt werden kann. Dies h√§tte keine spezielle Hardware au√üer einem Laptop ben√∂tigt, allerdings waren wir uns unsicher, wie gut wir die Software implementieren k√∂nnen, und bef√ºrchteten, dieser Audio-Basierte Input k√∂nnten schwer zu debuggen sein.

### Entscheidung

Aus diesem Pool verschiedener interessanter Interaktionstechniken haben wir uns letztlich f√ºr `"Design and Development of Hand Gesture Based Virtual Mouse" von Shibly et al. (ICASERT 2019)` entschieden. Gefallen an der L√∂sung hat uns, dass sie einen vollwertiger Maus-Ersatz darstellt mit allen wichtigen Funktionen.

Ein weiterer positiver Aspekt am Paper ist, dass die Programmstruktur & Gesten genau erkl√§rt und mit Bildern dokumentiert sind. An manchen Stellen h√§tten wir uns allerdings noch eine etwas genauere Erkl√§rung gew√ºnscht, beispielsweise zwecks den Erkl√§rungen √ºber Implementierung und Designentscheidung hinsichtlich den Gesten f√ºr Links-, Rechts- und Doppelklick.

 Auch die Erkennungsquoten der einzelnen Gestem im Paper waren mit durchschnittlich ca. 90% (vor wei√üem Hintergrund) relativ gut, was vielversprechend wirkte. Zus√§tzlich fanden wir es praktisch, dass als Materialaufwand lediglich etwas farbiger Tesa-Film und ggf. ein Handschuh, an dem man den Tesa festkleben kann, ben√∂tigt wird.

 Wir waren zuversichtlich, dass die Coding-Herausforderungen dieses Projektes unseren im Kurs erworbenen Kenntnissen entsprechen w√ºrde. Zudem ist diese Art von Anwendung leicht visuell zu debuggen, was bei der Entwicklung geholfen hat.

 Auch die potentiellen Anwendungszwecke der Gesture-Mouse fanden wir interessant. Neben dem Steuern von Computern nennt das Paper auch das Steuern von Robotern als potentiellen Anwendungszweck, oder als Steuerung in VR-Umgebungen, in denen man die H√§nde frei haben will. Auch f√ºr Menschen, die ihre Gliedma√üen nicht oder schlecht bewegen k√∂nnen, k√∂nnte das System hilfreich sein.

Abschlie√üend l√§sst sich sagen, dass das vorgeschlagene Interaktionssystem des Papers viel Potential bietet. Wir haben an manchen Stellen noch Erweiterungspotential und Quality-Of-Life improvements implementiert, wie eine automatische Kalibrierung, eine Puffer-Border um das Anwendungsfenster und Audio-Feedback.

---

## Designentscheidungen und Funktionsweise

### 1. Marker-Tracking & Kalibrierung

Wie im Referenzpaper erfolgt die Gestenerkennung Anhand des Verh√§ltnisses und der Bewegung von drei gleichfarbigen Markern an Zeigefinger, Mittelfinger und Daumen. Anders als im Paper, wo eine von 5 Farben ausgew√§hlt werden musste, wurde dieser Ansatz von uns durch die M√∂glichkeit, die zu erkennende Markerfarbe dynamisch zu kalibrieren, verbessert. Dies erh√∂ht auch die Erkennungsquote bei verschiedenen Lichtverh√§ltnissen.

- Farbbasierte Markerkennung mittels OpenCV im HSV-Farbraum.
- Die Kalibrierung der Markerfarbe erfolgt manuell √ºber die Taste `k`: Ein durchschnittlicher Farbwert wird √ºber 3 Sekunden (einstellbar) aus dem markierten Zentrum des Bildes bestimmt und rechts oben angezeigt. Anfangs wird ein Text als Hinweis auf die notwendige Kalibrierung angezeigt und vorgelesen.
- Je nach Anzahl und Dauer gleichzeitig erkannter, valider Konturen (richtige Farbe und Gr√∂√üe) werden verschiedene Gesten erkannt und entsprechende Aktionen ausgef√ºhrt.

### 2. Steuerung der Mausposition

Die Geste und ihre Anzeige (Roter Mittelpunkt zwischen zwei erkannten Markern = Mausposition, Anzeige von Bounding-Boxen und ihren Verbindungslinien) wurde direkt aus dem Referenzpaper genommen. Erg√§nzt wurde eine Einschr√§nkung des Tracking-Bereichs, damit auch die √§u√üersten Ecken des Bildschirms sicher angesteuert werden k√∂nnen.

- Berechnung eines Mauscursor-Trackingbereichs (gr√ºner Kasten) zum Anwendungsstart, der das Seitenverh√§ltnis des Bildschirms abbildet. Seine Gr√∂√üe richtet sich nach einem prozentual zur Seitenl√§nge definierten Abstand zum Fensterrand (einstellbar).
- Durch Minimale Blob-Gr√∂√üen und morphologischen Operationen werden die Fingerspitzen sehr gut erkannt. Gute Lichtverh√§ltnisse sind je nach Webcam-Qualit√§t trotzdem vorteilhaft.
- Die Mausposition wird dynamisch von Markerkoordinaten auf Bildschirmkoordinaten gemappt. Dabei wird sichergestellt, dass die Bewegung nicht verzerrt wird.
- Clipping verhindert Ausrei√üer √ºber den Bildschirmrand hinaus.

### 3. Klick- und Scroll-Gesten

Die Gesten, ihre Funktionsweisen und Anzeige (Anzeige von Bounding-Boxen und ihren Verbindungslinien) wurden direkt aus dem Referenzpaper entnommen, wenn auch als suboptimal erachtet (siehe Limitationen und Ergebnisse des Referenzpapers). F√ºr den Rechtsklick wurde eine Mindestanzahl zu haltenden Frames eingef√ºhrt. Sie verhindert st√§ndige, ungewollte Rechtsklicks durch flackern bei schlechter Markererkennung. Eine Anzeige der derzeit erkannten Geste als Text unten links inkl. Logging und Sprachausgabe wurde erg√§nzt (einstellbar).

- **Rechtsklick**: Ein einzelner Marker bleibt l√§nger als‚ÄØ8 Frames (einstellbar) im Bild. 
- **Linksklick**: Zwei Marker werden zusammengef√ºhrt ("Pinch") (Gemeinsame Bounding Box <= 20‚ÄØ% der Startfl√§che, einstellbar).
- **Doppel-Linksklick**: Der Pinch wird l√§nger als‚ÄØ5 Sekunden (einstellbar) gehalten
- **Scroll**: Drei Marker bewegen sich sichtbar nach oben oder unten (Vergleich des durchschnittlichen y-Werts mit Vorwert, n√∂tige Differenz einstellbar).

Ausgef√ºhrt werden die Gesten per `pynput`, um echte Maus-Eingaben auf dem Desktop t√§tigen zu k√∂nnen.

---

## Limitationen

- Die Farberkennung kann bei schlechten Lichtverh√§ltnissen oder reflektierenden Oberfl√§chen unzuverl√§ssig sein.
- Gegenst√§nde oder Hintergr√ºnde in Markerfarbe machen das System unbenutzbar (bei rot z.B. sogar Lippen).
- Ab einer gewissen N√§he werden zwei Marker als eine Fl√§che erkannt -> Rechtsklick statt Linksklick.
- Inkonsistente Erkennung kann dazu f√ºhren, dass ungewollt Rechtsklicke ausgef√ºhrt werden (Designentscheidung des originalen Papers).

---

## Verbesserungsvorschl√§ge

Die Anwendung funktioniert relativ zuverl√§ssig, allerdings w√ºrden wir R√ºckblickend ein paar Design-Entscheidungen, welche wir aus dem Paper √ºbernommen haben, √ºberdenken:

- Rechtsklick mit komplexerer Geste, um versehentlich ausgel√∂ste Gesten zu verringern.
- M√∂glicherweise w√§re die Linksklick-Geste weniger verwechselbar als Ein-Marker-Geste
- Entweder 4 statt 3 Marker, oder eine Zweite Farbe, z. B. auf dem Handr√ºcken, k√∂nnte f√ºr seltene Operationen praktisch sein